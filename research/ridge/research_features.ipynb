{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f105b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, json, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390a7080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths (adjust if your layout differs)\n",
    "ROOT = Path('/home/pduce/ICAIF_2025_Cryptocurrency_Forecasting_Starter_Kit')\n",
    "DATA = ROOT / \"data\"\n",
    "SRC  = ROOT / \"src\"\n",
    "SUBM = ROOT / \"sample_submission\"\n",
    "\n",
    "# Ensure src is importable\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC))\n",
    "\n",
    "# Create sample_submission dir if missing\n",
    "SUBM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda6a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_info.json loaded. Keys: ['freq', 'features', 'input_len', 'horizon_len', 'dtypes', 'outputs', 'sha256']\n",
      "{\n",
      "  \"features\": [\n",
      "    \"close\",\n",
      "    \"volume\"\n",
      "  ],\n",
      "  \"input_len\": 60,\n",
      "  \"horizon_len\": 10,\n",
      "  \"outputs\": {\n",
      "    \"train\": {\n",
      "      \"columns\": [\n",
      "        \"series_id\",\n",
      "        \"time_step\",\n",
      "        \"close\",\n",
      "        \"volume\"\n",
      "      ]\n",
      "    },\n",
      "    \"x_test\": {\n",
      "      \"columns\": [\n",
      "        \"window_id\",\n",
      "        \"time_step\",\n",
      "        \"close\",\n",
      "        \"volume\"\n",
      "      ]\n",
      "    },\n",
      "    \"y_test_local\": {\n",
      "      \"columns\": [\n",
      "        \"window_id\",\n",
      "        \"time_step\",\n",
      "        \"close\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "train shape: (18331224, 5) | columns: ['series_id', 'time_step', 'close', 'volume', 'event_datetime']\n",
      "x_test  shape: (3000000, 4) | columns: ['window_id', 'time_step', 'close', 'volume']\n",
      "y_test_local shape: (20, 3) | columns: ['window_id', 'time_step', 'close']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>event_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.13700</td>\n",
       "      <td>171985.703125</td>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.13656</td>\n",
       "      <td>85451.398438</td>\n",
       "      <td>2024-01-01 00:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.13647</td>\n",
       "      <td>121151.898438</td>\n",
       "      <td>2024-01-01 00:02:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   series_id  time_step    close         volume      event_datetime\n",
       "0          1          0  0.13700  171985.703125 2024-01-01 00:00:00\n",
       "1          1          1  0.13656   85451.398438 2024-01-01 00:01:00\n",
       "2          1          2  0.13647  121151.898438 2024-01-01 00:02:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>24976.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>2299.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window_id  time_step   close   volume\n",
       "0          1          0  0.1126  24976.0\n",
       "1          1          1  0.1126      0.0\n",
       "2          1          2  0.1125   2299.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window_id  time_step   close\n",
       "0          1          0  0.1131\n",
       "1          1          1  0.1131\n",
       "2          1          2  0.1130"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset files\n",
    "info_path = DATA / \"dataset_info.json\"\n",
    "if info_path.exists():\n",
    "    info = json.loads(info_path.read_text(encoding=\"utf-8\"))\n",
    "    print(\"dataset_info.json loaded. Keys:\", list(info.keys()))\n",
    "    print(json.dumps({k: info[k] for k in ['features','input_len','horizon_len','outputs']}, indent=2))\n",
    "else:\n",
    "    print(\"dataset_info.json not found at\", info_path)\n",
    "\n",
    "# Peek train / x_test\n",
    "train_path = DATA / \"train.pkl\"\n",
    "x_test_path  = DATA / \"x_test.pkl\"\n",
    "y_local_path = DATA / \"y_test_local.pkl\"\n",
    "\n",
    "train = pd.read_pickle(train_path)\n",
    "train['event_datetime'] = pd.to_datetime('2024-01-01') + train['time_step']*pd.Timedelta(minutes=1) \n",
    "x_test  = pd.read_pickle(x_test_path)\n",
    "y_test_local = pd.read_pickle(y_local_path)\n",
    "\n",
    "print(\"train shape:\", train.shape, \"| columns:\", train.columns.tolist())\n",
    "print(\"x_test  shape:\", x_test.shape,  \"| columns:\", x_test.columns.tolist())\n",
    "print(\"y_test_local shape:\", y_test_local.shape, \"| columns:\", y_test_local.columns.tolist())\n",
    "\n",
    "display(train.head(3))\n",
    "display(x_test.head(3))\n",
    "display(y_test_local.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4339ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.lib.stride_tricks import sliding_window_view as swv\n",
    "\n",
    "class WindowsDatasetVect:\n",
    "    \"\"\"\n",
    "    Vectorized window builder that returns two DataFrames:\n",
    "\n",
    "    X columns:\n",
    "      - window_id: integer id of each window\n",
    "      - time_step: 0..(input_len-1) within the input segment\n",
    "      - close\n",
    "      - volume\n",
    "      - event_datetime\n",
    "\n",
    "    y columns:\n",
    "      - window_id: same ids as in X\n",
    "      - time_step: 0..(horizon_len-1) within the future horizon\n",
    "      - close: future close\n",
    "      - prev_close: the last input close (index input_len-1, e.g. 59 when input_len=60)\n",
    "      - event_datetime: timestamps of the future horizon\n",
    "\n",
    "    Notes:\n",
    "      * Requires df to have columns:\n",
    "          ['series_id','time_step','close','volume','event_datetime']\n",
    "      * Windows are created per series_id after sorting by time_step.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        df : pd.DataFrame = None,\n",
    "        train_path: str = None,\n",
    "        window: int = 70,\n",
    "        input_len: int = 60,\n",
    "        horizon_len: int = 10,\n",
    "        rolling: bool = True,\n",
    "        step_size: int | None = None,\n",
    "    ) -> None:\n",
    "        assert input_len + horizon_len == window, \"window must equal input_len + horizon_len\"\n",
    "        # default stepping: 1 if rolling, else full non-overlapping windows\n",
    "        if step_size is None:\n",
    "            step_size = 1 if rolling else window\n",
    "\n",
    "        if train_path is not None:\n",
    "            df = pd.read_pickle(train_path)\n",
    "        if df is None:\n",
    "            raise ValueError(\"Provide either df or train_path\")\n",
    "\n",
    "        required = {'series_id','time_step','close','volume','event_datetime'}\n",
    "        if not required.issubset(df.columns):\n",
    "            raise ValueError(f\"df missing required columns {required}, found {list(df.columns)}\")\n",
    "\n",
    "        # group per series, sorted by time_step\n",
    "        groups = {sid: g.sort_values('time_step').reset_index(drop=True)\n",
    "                  for sid, g in df.groupby('series_id')}\n",
    "\n",
    "        X_parts: list[pd.DataFrame] = []\n",
    "        Y_parts: list[pd.DataFrame] = []\n",
    "\n",
    "        next_win_id = 0\n",
    "\n",
    "        for _, g in groups.items():\n",
    "            n = len(g)\n",
    "            if n < window:\n",
    "                continue\n",
    "\n",
    "            close  = g['close' ].to_numpy(np.float32)\n",
    "            volume = g['volume'].to_numpy(np.float32)\n",
    "            dt     = g['event_datetime'].to_numpy('datetime64[ns]')\n",
    "\n",
    "            # sliding windows (shape: (n - window + 1, window))\n",
    "            w_close  = swv(close,  window_shape=window)[::step_size]\n",
    "            w_volume = swv(volume, window_shape=window)[::step_size]\n",
    "            w_dt     = swv(dt,     window_shape=window)[::step_size]\n",
    "\n",
    "            num_win = w_close.shape[0]\n",
    "            if num_win == 0:\n",
    "                continue\n",
    "\n",
    "            # split into input and horizon\n",
    "            x_close  = w_close[:,  :input_len]                      # (num_win, input_len)\n",
    "            x_volume = w_volume[:, :input_len]\n",
    "            x_dt     = w_dt[:,     :input_len]\n",
    "\n",
    "            y_close = w_close[:,  input_len:]                       # (num_win, horizon_len)\n",
    "            y_dt    = w_dt[:,     input_len:]\n",
    "            prev_c  = x_close[:, -1]                                # (num_win,)\n",
    "\n",
    "            # window ids for this group\n",
    "            win_ids = np.arange(next_win_id, next_win_id + num_win, dtype=np.int64)\n",
    "\n",
    "            # X dataframe chunk\n",
    "            X_parts.append(pd.DataFrame({\n",
    "                \"window_id\":     np.repeat(win_ids, input_len)+1,\n",
    "                \"time_step\":     np.tile(np.arange(input_len, dtype=np.int32), num_win),\n",
    "                \"close\":         x_close.reshape(-1),\n",
    "                \"volume\":        x_volume.reshape(-1),\n",
    "                \"event_datetime\": x_dt.reshape(-1),\n",
    "            }))\n",
    "\n",
    "            # y dataframe chunk\n",
    "            Y_parts.append(pd.DataFrame({\n",
    "                \"window_id\":     np.repeat(win_ids, horizon_len)+1,\n",
    "                \"time_step\":     np.tile(np.arange(horizon_len, dtype=np.int32), num_win),\n",
    "                \"close\":         y_close.reshape(-1),\n",
    "                \"prev_close\":    np.repeat(prev_c, horizon_len),\n",
    "                \"event_datetime\": y_dt.reshape(-1),\n",
    "            }))\n",
    "\n",
    "            next_win_id += num_win\n",
    "\n",
    "        # Public attributes\n",
    "        if X_parts:\n",
    "            self.X = pd.concat(X_parts, ignore_index=True)\n",
    "        else:\n",
    "            self.X = pd.DataFrame(columns=[\"window_id\",\"time_step\",\"close\",\"volume\",\"event_datetime\"])\n",
    "\n",
    "        if Y_parts:\n",
    "            self.y = pd.concat(Y_parts, ignore_index=True)\n",
    "        else:\n",
    "            self.y = pd.DataFrame(columns=[\"window_id\",\"time_step\",\"close\",\"prev_close\",\"event_datetime\"])\n",
    "\n",
    "        self.num_windows = int(self.X[\"window_id\"].max() + 1) if len(self.X) else 0\n",
    "        self.input_len = input_len\n",
    "        self.horizon_len = horizon_len\n",
    "        self.window = window\n",
    "        self.step_size = step_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_windows\n",
    "\n",
    "    def windows(self, window_id: int) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Convenience: return (X_rows, y_rows) for a given window_id.\"\"\"\n",
    "        Xw = self.X[self.X[\"window_id\"] == window_id].sort_values(\"time_step\")\n",
    "        Yw = self.y[self.y[\"window_id\"] == window_id].sort_values(\"time_step\")\n",
    "        return Xw, Yw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab68233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW / UPDATED CODE\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List, Optional, Iterable, Callable, Tuple\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# --- your existing imports ---\n",
    "from icaif.dataset import TrainWindowSampler, TrainWindowSamplerVect\n",
    "from athenea.stats.regressions import Ridge\n",
    "from icaif.metrics import evaluate_all_metrics\n",
    "\n",
    "SEED = 42  # ensure you define this somewhere\n",
    "\n",
    "def transform(X):\n",
    "    \"\"\"\n",
    "    X: array-like of shape (n_samples, 60, 2)\n",
    "       [:, :, 0] = prices (close); [:, :, 1] = volumes\n",
    "    Returns: list[pd.DataFrame], each of shape (n_samples, 1)\n",
    "    \"\"\"\n",
    "    EPS = 1e-12\n",
    "\n",
    "    # Arrange as (time x samples)\n",
    "    X_prices  = pd.DataFrame(X[:, :, 0]).T\n",
    "    X_volumes = pd.DataFrame(X[:, :, 1]).T\n",
    "\n",
    "    logp    = np.log(X_prices)\n",
    "    logrets = logp.diff()  # 1-min log returns, time on rows\n",
    "\n",
    "    # ----- Your original features (named) -----\n",
    "    avg_lr        = logrets.mean().to_frame()\n",
    "    sign_change_share = np.sign(logrets).diff().ne(0).sum().to_frame()\n",
    "    avg_vol_lr = logrets.mul(X_volumes,axis=0).mean().to_frame()\n",
    "    vd = np.sign(logrets).mul(X_volumes,axis=0).mean().to_frame()\n",
    "    rv = logrets.pow(2).mean().to_frame()\n",
    "\n",
    "    r_5 = logrets.iloc[-5].to_frame(0)\n",
    "    r_10 = logrets.iloc[-10].to_frame(0)\n",
    "    r_20 = logrets.iloc[-20].to_frame(0)\n",
    "    r_30 = logrets.iloc[-30].to_frame(0)\n",
    "    r_40 = logrets.iloc[-40].to_frame(0)\n",
    "    r_50 = logrets.iloc[-50].to_frame(0)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    features = [\n",
    "        avg_lr,\n",
    "        avg_vol_lr,\n",
    "        vd,\n",
    "        rv,\n",
    "        r_5,\n",
    "        r_10,\n",
    "        r_20,\n",
    "        r_30,\n",
    "        r_40,\n",
    "        r_50,\n",
    "    ]\n",
    "\n",
    "    #sig = jax_gpu_signature(X, depth=3)\n",
    "    #df_sig = pd.DataFrame(sig)\n",
    "    #print(df_sig.head())\n",
    "    return features\n",
    "\n",
    "from features_compute import build_features_np\n",
    "\n",
    "def transform_nick(X):\n",
    "    return [pd.DataFrame(f) for f in build_features_np(X)]\n",
    "\n",
    "from icaif.metrics_np import evaluate_all_metrics_vectorized\n",
    "\n",
    "def evaluate(model_results, y_true):\n",
    "\n",
    "    y_pred = y_true.copy(deep=True)\n",
    "    y_pred['pred_close'] = np.exp(model_results.loc[model_results.index.repeat(10)].groupby(level=0).cumsum()).reset_index(drop=True)\n",
    "    y_pred['pred_close'] *= y_pred['prev_close']\n",
    "\n",
    "    results = evaluate_all_metrics_vectorized(\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "# --------------- NEW: worker function ---------------\n",
    "def _run_one_fold(\n",
    "    idx: int,\n",
    "    train_ids: np.ndarray,\n",
    "    val_ids: np.ndarray,\n",
    "    df_train: pd.DataFrame,\n",
    "    model_or_factory: Any,\n",
    ") -> Tuple[int, str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Execute a single fold end-to-end and return (idx, fold_name, metrics).\n",
    "    Runs in a separate process when parallelized.\n",
    "    \"\"\"\n",
    "    # Rebuild model per process\n",
    "    model = model_or_factory() if callable(model_or_factory) else copy.deepcopy(model_or_factory)\n",
    "\n",
    "    # Slice data for this fold\n",
    "    df_tr = df_train[df_train['series_id'].isin(train_ids)].copy()\n",
    "    df_va = df_train[df_train['series_id'].isin(val_ids)].copy()\n",
    "\n",
    "    # Defensive: skip degenerate folds\n",
    "    if df_tr.empty or df_va.empty:\n",
    "        return idx, f\"val_{'-'.join(map(str, val_ids))}\", {}\n",
    "\n",
    "    train_ds = WindowsDatasetVect(df_tr)\n",
    "    val_ds = WindowsDatasetVect(df_va)\n",
    "\n",
    "    X_train = train_ds.X\n",
    "    y_train = train_ds.y\n",
    "\n",
    "    X_val = val_ds.X\n",
    "    y_val = val_ds.y\n",
    "\n",
    "    X_np_train = X_train.set_index(['window_id','time_step'])[['close','volume']].to_numpy().reshape(-1, 60, 2)\n",
    "    y_np_train = y_train.set_index(['window_id','time_step'])[['close']].to_numpy().reshape(-1, 10)\n",
    "\n",
    "    X_np_val = X_val.set_index(['window_id','time_step'])[['close','volume']].to_numpy().reshape(-1, 60, 2)\n",
    "    y_np_val = y_val.set_index(['window_id','time_step'])[['close']].to_numpy().reshape(-1, 10)\n",
    "\n",
    "    y_lr_train = pd.Series(np.diff(np.log(y_np_train),axis=1).mean(axis=1)).to_frame()\n",
    "\n",
    "    features = model.transform(X_np_train)\n",
    "    features_val = model.transform(X_np_val)\n",
    "\n",
    "    model.fit(y_lr_train, features)\n",
    "    y_pred = model.predict(features_val)\n",
    "\n",
    "    metrics: Dict[str, Any] = model.evaluate(y_pred, y_val)\n",
    "\n",
    "    fold_name = f\"val_{'-'.join(map(str, val_ids))}\"\n",
    "    return idx, fold_name, metrics\n",
    "\n",
    "\n",
    "# --------------- UPDATED: run_cv with parallelism ---------------\n",
    "def run_cv(\n",
    "    df_train: pd.DataFrame,\n",
    "    n_train: int = 4,\n",
    "    n_val: int = 1,\n",
    "    model=None,\n",
    "    *,\n",
    "    include_last_fold: bool = False,   # set True to include the final possible window\n",
    "    n_jobs: int = 1,                   # NEW: #processes; 1 keeps it sequential\n",
    "    model_factory: Optional[Callable[[], Any]] = None,  # NEW: pass to rebuild model per fold\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rolling group-based CV, optionally parallel across folds (process-based).\n",
    "    Assumptions about `model` / `model_factory`:\n",
    "      - Either:\n",
    "          model_factory() -> fresh model with fit/predict/evaluate/transform\n",
    "        Or:\n",
    "          model is a picklable object with those methods (we deep-copy it per fold).\n",
    "    \"\"\"\n",
    "    if (model is None) and (model_factory is None):\n",
    "        raise ValueError(\"Provide either `model` or a `model_factory` (callable returning a fresh model).\")\n",
    "\n",
    "    series_ids = df_train['series_id'].unique()\n",
    "    n_series   = len(series_ids)\n",
    "    n_total    = n_train + n_val\n",
    "\n",
    "    if n_total > n_series:\n",
    "        raise ValueError(f\"n_train + n_val must be ≤ number of groups ({n_series}).\")\n",
    "    if n_train < 1:\n",
    "        raise ValueError(f\"n_train must be ≥ 1 (got {n_train}).\")\n",
    "    if n_val < 1:\n",
    "        raise ValueError(f\"n_val must be ≥ 1 (got {n_val}).\")\n",
    "    if n_train < n_val:\n",
    "        raise ValueError(f\"n_train must be ≥ n_val (got {n_train} < {n_val}).\")\n",
    "\n",
    "    start = n_total\n",
    "    stop  = n_series + (1 if include_last_fold else 0)\n",
    "\n",
    "    # Build fold definitions once\n",
    "    fold_specs = []\n",
    "    for i in range(start, stop):\n",
    "        train_ids = series_ids[i - n_total : i - n_val]\n",
    "        val_ids   = series_ids[i - n_val   : i]\n",
    "        fold_specs.append((i, train_ids, val_ids))\n",
    "\n",
    "    if not fold_specs:\n",
    "        raise RuntimeError(\"No folds were produced. Check your data and parameters.\")\n",
    "\n",
    "    # Choose what to pass to workers for model creation\n",
    "    model_or_factory = model_factory if model_factory is not None else model\n",
    "\n",
    "    # Sequential path (n_jobs == 1)\n",
    "    if n_jobs == 1:\n",
    "        results = [\n",
    "            _run_one_fold(idx, train_ids, val_ids, df_train, model_or_factory)\n",
    "            for (idx, train_ids, val_ids) in fold_specs\n",
    "        ]\n",
    "    else:\n",
    "        # Parallel path (processes)\n",
    "        # Tip: to avoid CPU over-subscription with BLAS, consider setting env vars:\n",
    "        # OMP_NUM_THREADS=1 MKL_NUM_THREADS=1 NUMEXPR_NUM_THREADS=1\n",
    "        results = [None] * len(fold_specs)\n",
    "        with ProcessPoolExecutor(max_workers=n_jobs) as ex:\n",
    "            futures = {\n",
    "                ex.submit(_run_one_fold, idx, train_ids, val_ids, df_train, model_or_factory): pos\n",
    "                for pos, (idx, train_ids, val_ids) in enumerate(fold_specs)\n",
    "            }\n",
    "            for fut in as_completed(futures):\n",
    "                pos = futures[fut]\n",
    "                results[pos] = fut.result()\n",
    "\n",
    "    # results: list of (idx, fold_name, metrics)\n",
    "    # Keep chronological order by fold index\n",
    "    results.sort(key=lambda t: t[0])\n",
    "    fold_names = [name for _, name, _ in results]\n",
    "    metric_rows = [metrics for _, _, metrics in results]\n",
    "\n",
    "    # Assemble DataFrame: rows = metric names, cols = folds\n",
    "    metrics_df = pd.DataFrame(metric_rows, index=fold_names).T\n",
    "    return metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e8a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from athenea.stats.regressions import Ridge\n",
    "def make_ridge(l2=0.1):\n",
    "    model = Ridge(l2=l2)\n",
    "    model.transform = transform\n",
    "    model.evaluate = evaluate\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics = run_cv(\n",
    "    df_train=train,\n",
    "    n_train=10,\n",
    "    n_val=2,\n",
    "    include_last_fold=True,\n",
    "    n_jobs=1,\n",
    "    model_factory=make_ridge(l2=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1de80c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSE            0.000048\n",
       "MAE            0.002885\n",
       "IC             0.027106\n",
       "IR             0.090055\n",
       "SharpeRatio    0.033906\n",
       "MDD            0.946407\n",
       "VaR           -0.004963\n",
       "ES            -0.008986\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51be70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSE            0.000009\n",
       "MAE            0.000630\n",
       "IC             0.092503\n",
       "IR             0.157267\n",
       "SharpeRatio    0.090816\n",
       "MDD            0.957867\n",
       "VaR           -0.008703\n",
       "ES            -0.018601\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "964a85bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSE            0.000009\n",
       "MAE            0.000628\n",
       "IC             0.068642\n",
       "IR             0.126262\n",
       "SharpeRatio    0.080043\n",
       "MDD            0.963835\n",
       "VaR           -0.008796\n",
       "ES            -0.018692\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0de4abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = val_ds.y.copy(deep=True)\n",
    "y_true['pred_close'] = np.exp(y_pred.loc[y_pred.index.repeat(10)].groupby(level=0).cumsum()).reset_index(drop=True)\n",
    "y_true['pred_close'] *= y_true['prev_close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e4ea046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MSE': 7.324066618496394e-06,\n",
       " 'MAE': 0.0006133882334464747,\n",
       " 'IC': 0.04474149733947167,\n",
       " 'IR': 0.07659221911983606,\n",
       " 'SharpeRatio': 0.07708397267707696,\n",
       " 'MDD': 0.9547024643339382,\n",
       " 'VaR': -0.008717975181701603,\n",
       " 'ES': -0.018549726934457684}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from icaif.metrics_np import evaluate_all_metrics_vectorized\n",
    "\n",
    "evaluate_all_metrics_vectorized(\n",
    "    y_true=val_ds.y,\n",
    "    y_pred=y_true,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
